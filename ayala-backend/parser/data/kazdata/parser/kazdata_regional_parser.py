from playwright.sync_api import sync_playwright
import csv
import time
import os
import re
from typing import Dict, List

class KazDataRegionalParser:
    """
    –ï–¥–∏–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Å–µ–º —Ä–µ–≥–∏–æ–Ω–∞–º –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞ —Å KazDATA
    """
    
    def __init__(self):
        # –í—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ URL —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ö–†–ü –∫–æ–¥–∞–º–∏
        self.regional_urls = {
            'pavlodar': [
                'https://kazdata.kz/04/2015-kazakhstan-pavlodar-i-oblast-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-pavlodar-i-oblast-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-pavlodar-i-oblast-305.html'
            ],
            'almaty_oblast': [
                'https://kazdata.kz/04/2015-kazakhstan-almaty-oblast-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-almaty-oblast-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-almaty-oblast-305.html',
                'https://kazdata.kz/04/2015-kazakhstan-almaty-i-oblast-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-taldykorgan-i-oblast-311.html'
            ],
            'astana': [
                'https://kazdata.kz/04/2015-kazakhstan-astana-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-astana-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-astana-305.html'
            ],
            'almaty_city': [
                'https://kazdata.kz/04/2015-kazakhstan-almaty-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-almaty-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-almaty-305.html'
            ],
            'shymkent': [
                'https://kazdata.kz/04/2015-kazakhstan-shymkent-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-shymkent-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-shymkent-305.html'
            ],
            'karaganda': [
                'https://kazdata.kz/04/2015-kazakhstan-karaganda-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-karaganda-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-karaganda-305.html'
            ],
            'aktobe': [
                'https://kazdata.kz/04/2015-kazakhstan-aktobe-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-aktobe-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-aktobe-305.html'
            ],
            'atyrau': [
                'https://kazdata.kz/04/2015-kazakhstan-atyrau-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-atyrau-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-atyrau-305.html'
            ],
            'kostanay': [
                'https://kazdata.kz/04/2015-kazakhstan-kostanay-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-kostanay-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-kostanay-305.html'
            ],
            'mangistau': [
                'https://kazdata.kz/04/2015-kazakhstan-aktau-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-aktau-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-aktau-305.html'
            ],
            'semey': [
                'https://kazdata.kz/04/2015-kazakhstan-semey-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-semey-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-semey-305.html'
            ],
            'oral': [
                'https://kazdata.kz/04/2015-kazakhstan-oral-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-oral-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-oral-305.html'
            ],
            'taraz': [
                'https://kazdata.kz/04/2015-kazakhstan-taraz-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-taraz-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-taraz-305.html'
            ],
            'kokshetau': [
                'https://kazdata.kz/04/2015-kazakhstan-kokshetau-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-kokshetau-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-kokshetau-305.html'
            ],
            'petropavlovsk': [
                'https://kazdata.kz/04/2015-kazakhstan-petropavlovsk-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-petropavlovsk-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-petropavlovsk-305.html'
            ],
            'turkestan': [
                'https://kazdata.kz/04/2015-kazakhstan-turkestan-311.html',
                'https://kazdata.kz/04/2015-kazakhstan-turkestan-310.html',
                'https://kazdata.kz/04/2015-kazakhstan-turkestan-305.html'
            ]
        }
        
        self.companies_by_region = {}
    
    def parse_single_page(self, url: str, region: str, browser) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π
        """
        companies = []
        page = browser.new_page()
        
        page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        try:
            print(f"    –ü–∞—Ä—Å–∏–Ω–≥: {url}")
            page.goto(url, timeout=30000)
            time.sleep(2)
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
            selectors_to_try = [
                'table.sp tbody tr',
                'table.sp tr',
                'table tbody tr',
                'table tr',
                '.sp tbody tr',
                '.sp tr'
            ]
            
            for selector in selectors_to_try:
                try:
                    rows = page.locator(selector).all()
                    if len(rows) > 1:
                        companies_found = 0
                        
                        for row in rows:
                            try:
                                cells = row.locator('td, th').all()
                                
                                if len(cells) >= 8:
                                    bin_text = cells[0].inner_text().strip()
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ë–ò–ù (—Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, –¥–ª–∏–Ω–∞ >= 10)
                                    if bin_text.isdigit() and len(bin_text) >= 10:
                                        company_data = {
                                            'bin': bin_text,
                                            'name': cells[1].inner_text().strip(),
                                            'oked': cells[2].inner_text().strip(),
                                            'industry': cells[3].inner_text().strip(),
                                            'kato': cells[4].inner_text().strip(),
                                            'settlement': cells[5].inner_text().strip(),
                                            'krp': cells[6].inner_text().strip(),
                                            'company_size': cells[7].inner_text().strip().replace('\n', ' '),
                                            'region': region,
                                            'source_url': url
                                        }
                                        
                                        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                                        for key, value in company_data.items():
                                            if isinstance(value, str):
                                                company_data[key] = ' '.join(value.split())
                                        
                                        companies.append(company_data)
                                        companies_found += 1
                                        
                            except Exception:
                                continue
                        
                        if companies_found > 0:
                            print(f"      ‚úÖ –ù–∞–π–¥–µ–Ω–æ {companies_found} –∫–æ–º–ø–∞–Ω–∏–π")
                            break
                            
                except Exception:
                    continue
            
            if not companies:
                print(f"      ‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}...")
        finally:
            page.close()
        
        return companies
    
    def parse_all_regions(self):
        """
        –ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã
        """
        print("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö KazDATA")
        print("=" * 70)
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            
            try:
                total_regions_processed = 0
                total_companies_found = 0
                
                for region, urls in self.regional_urls.items():
                    print(f"\nüìç –†–µ–≥–∏–æ–Ω: {region.upper()}")
                    region_companies = []
                    
                    for url in urls:
                        companies = self.parse_single_page(url, region, browser)
                        region_companies.extend(companies)
                        
                        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        time.sleep(1)
                    
                    if region_companies:
                        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ë–ò–ù
                        unique_companies = {}
                        for company in region_companies:
                            bin_number = company['bin']
                            if bin_number not in unique_companies:
                                unique_companies[bin_number] = company
                        
                        region_companies = list(unique_companies.values())
                        self.companies_by_region[region] = region_companies
                        
                        print(f"  ‚úÖ –ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è {region}: {len(region_companies)}")
                        total_companies_found += len(region_companies)
                        total_regions_processed += 1
                    else:
                        print(f"  ‚ùå –î–∞–Ω–Ω—ã–µ –¥–ª—è {region} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
                print(f"\n" + "=" * 70)
                print(f"üéØ –ò–¢–û–ì–ò –ü–ê–†–°–ò–ù–ì–ê:")
                print(f"   –†–µ–≥–∏–æ–Ω–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_regions_processed}")
                print(f"   –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ: {total_companies_found}")
                
            finally:
                browser.close()
    
    def save_regional_data(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π CSV —Ñ–∞–π–ª
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é kazdata
        kazdata_dir = 'kazdata'
        os.makedirs(kazdata_dir, exist_ok=True)
        
        fieldnames = [
            'bin', 'name', 'oked', 'industry', 'kato', 
            'settlement', 'krp', 'company_size', 'region', 'source_url'
        ]
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é /{kazdata_dir}/")
        print("-" * 50)
        
        total_files_created = 0
        
        for region, companies in self.companies_by_region.items():
            if companies:
                filename = f"{kazdata_dir}/{region}_large_companies.csv"
                
                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(companies)
                
                print(f"‚úÖ {region}_large_companies.csv - {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")
                total_files_created += 1
        
        # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Å–≤–æ–¥–Ω—ã–π —Ñ–∞–π–ª
        if self.companies_by_region:
            all_companies = []
            for companies in self.companies_by_region.values():
                all_companies.extend(companies)
            
            summary_filename = f"{kazdata_dir}/all_regions_summary.csv"
            with open(summary_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_companies)
            
            print(f"‚úÖ all_regions_summary.csv - {len(all_companies)} –∫–æ–º–ø–∞–Ω–∏–π (—Å–≤–æ–¥–Ω—ã–π)")
            total_files_created += 1
        
        print(f"\nüìä –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files_created}")
        return total_files_created
    
    def show_statistics(self):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        """
        if not self.companies_by_region:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∫–∞–∑–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
            return
        
        print(f"\nüìà –ü–û–î–†–û–ë–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 70)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–≥–∏–æ–Ω—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–æ–º–ø–∞–Ω–∏–π
        sorted_regions = sorted(
            self.companies_by_region.items(), 
            key=lambda x: len(x[1]), 
            reverse=True
        )
        
        for region, companies in sorted_regions:
            print(f"{region.ljust(20)} | {len(companies):>4} –∫–æ–º–ø–∞–Ω–∏–π")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π
        krp_stats = {}
        for companies in self.companies_by_region.values():
            for company in companies:
                krp = company.get('krp', 'unknown')
                krp_stats[krp] = krp_stats.get(krp, 0) + 1
        
        print(f"\nüìä –ü–æ —Ä–∞–∑–º–µ—Ä–∞–º –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π (–ö–†–ü):")
        for krp, count in sorted(krp_stats.items()):
            krp_name = {
                '311': '–ö—Ä—É–ø–Ω—ã–µ (1001+ —á–µ–ª.)',
                '310': '–ö—Ä—É–ø–Ω—ã–µ (501-1000 —á–µ–ª.)', 
                '305': '–ö—Ä—É–ø–Ω—ã–µ (251-500 —á–µ–ª.)'
            }.get(krp, f'–ö–†–ü {krp}')
            print(f"  {krp_name}: {count} –∫–æ–º–ø–∞–Ω–∏–π")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    parser = KazDataRegionalParser()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö KazDATA")
    print("–ë—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã —Ñ–æ—Ä–º–∞—Ç–∞: {region}_large_companies.csv")
    print("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: /kazdata/")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
    parser.parse_all_regions()
    
    if parser.companies_by_region:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        files_created = parser.save_regional_data()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        parser.show_statistics()
        
        print(f"\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é /kazdata/ - —Å–æ–∑–¥–∞–Ω–æ {files_created} —Ñ–∞–π–ª–æ–≤")
    else:
        print("\n‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã. –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ —Å–∞–π—Ç—É.")

if __name__ == "__main__":
    main()

def parse_kyzylorda_companies(html_data):
    """
    Parses company data from Kyzylorda region HTML and returns a list of company dictionaries
    """
    companies = []
    
    # Split the HTML into company blocks
    company_blocks = html_data.split('<li class="flex flex-col border-b py-2">')
    
    for block in company_blocks[1:]:  # Skip first empty element
        try:
            company = {}
            
            # Extract company name
            name_start = block.find('<h2>') + 4
            name_end = block.find('</h2>')
            if name_start > 4 and name_end != -1:
                company['name'] = block[name_start:name_end].strip()
            
            # Extract address
            addr_start = block.find('<p class="text-sm">') + 18
            addr_end = block.find('</p>', addr_start)
            if addr_start > 18 and addr_end != -1:
                company['address'] = block[addr_start:addr_end].strip()
            
            # Extract activity
            activity_start = block.find('<div class="text-sm gap-1">') + 26
            activity_end = block.find('</div>', activity_start)
            if activity_start > 26 and activity_end != -1:
                company['activity'] = block[activity_start:activity_end].strip()
            
            # Extract manager
            manager_start = block.find('text-statsnet flex gap-1 flex-wrap">') + 35
            if manager_start > 35:
                manager_end = block.find('<span class="text-black">', manager_start)
                if manager_end != -1:
                    company['manager'] = block[manager_start:manager_end].strip()
            
            # Extract company size
            size_start = block.find('–ú–∞–ª–æ–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ')
            if size_start != -1:
                size_text = block[size_start:].split('</p>')[0].strip()
                company['company_size'] = size_text
            
            # Extract BIN
            bin_start = block.find('–ë–ò–ù') + 3
            if bin_start > 3:
                bin_text = block[bin_start:].split('</span>')[0].strip()
                company['bin'] = bin_text
            
            # Extract status (active/inactive)
            if 'ui-status--green' in block:
                company['status'] = '–ê–∫—Ç–∏–≤–Ω–∞—è'
            elif 'ui-status--red' in block:
                company['status'] = '–ù–µ–∞–∫—Ç–∏–≤–Ω–∞—è'
            
            # Extract company link
            link_start = block.find('href="') + 6
            link_end = block.find('"', link_start)
            if link_start > 6 and link_end != -1:
                company['link'] = f"https://statsnet.co{block[link_start:link_end]}"
            
            # Clean all text fields
            for key, value in company.items():
                if isinstance(value, str):
                    # Remove extra characters and normalize whitespace
                    cleaned_value = value.replace('>"', '').replace('">', '').replace('>', '')
                    company[key] = ' '.join(cleaned_value.split())
            
            if company.get('name') and company.get('bin'):  # Only add if we have at least name and BIN
                companies.append(company)
                
        except Exception as e:
            print(f"Error parsing company block: {e}")
            continue
    
    return companies

def save_to_csv(companies, filename):
    """
    Saves the parsed company data to a CSV file
    """
    # Create data directory if it doesn't exist
    os.makedirs('data', exist_ok=True)
    
    fieldnames = ['name', 'address', 'activity', 'manager', 'company_size', 'bin', 'status', 'link']
    
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(companies)
    
    print(f"Data saved to {filename}")
    print(f"Total companies: {len(companies)}") 