from playwright.sync_api import sync_playwright
import csv
import time
import os
import re
from typing import List, Dict
from urllib.parse import urljoin

class KazDataCatalogParser:
    def __init__(self):
        self.base_url = "https://kazdata.kz"
        self.catalog_url = "https://kazdata.kz/04/katalog-kazakhstan.html"
        self.companies_by_region = {}
        
    def extract_region_links(self, page) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞
        """
        region_links = {}
        
        try:
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            page.wait_for_selector('a[href*="kazakhstan"]', timeout=10000)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
            links = page.query_selector_all('a[href*="kazakhstan"]')
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
            
            for link in links:
                try:
                    href = link.get_attribute('href')
                    text = link.inner_text().strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏
                    if href and ('311.html' in href or '310.html' in href or '305.html' in href):
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL —Å /04/ –≤ –ø—É—Ç–∏
                        if not href.startswith('http'):
                            if href.startswith('/'):
                                href = href[1:]  # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–µ—à –µ—Å–ª–∏ –µ—Å—Ç—å
                            if not href.startswith('04/'):
                                href = f"04/{href}"
                            full_url = urljoin(self.base_url, href)
                        else:
                            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–ª–Ω—ã–π URL, –¥–æ–±–∞–≤–ª—è–µ–º /04/ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                            if '/04/' not in href:
                                parts = href.split('/')
                                domain = '/'.join(parts[:3])  # http://example.com
                                path = '/'.join(parts[3:])    # rest/of/path
                                full_url = f"{domain}/04/{path}"
                            else:
                                full_url = href
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑ URL
                        region_match = re.search(r'kazakhstan-([a-z-]+)-\d{3}', href)
                        if region_match:
                            region = region_match.group(1)
                            if region not in region_links:
                                region_links[region] = []
                            if full_url not in region_links[region]:
                                region_links[region].append(full_url)
                                print(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –¥–ª—è {region}: {full_url}")
                
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–∫–∏: {e}")
                    continue
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {e}")
        
        return region_links

    def parse_company_table(self, page) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏–π
        """
        companies = []
        
        try:
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Ç–∞–±–ª–∏—Ü—ã
            page.wait_for_selector('table', timeout=15000)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            tables = page.query_selector_all('table')
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(tables)}")
            
            for table in tables:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–∞–±–ª–∏—Ü–∞ —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 8 —Å—Ç–æ–ª–±—Ü–æ–≤)
                    header_cells = table.query_selector_all('tr:first-child td, tr:first-child th')
                    if len(header_cells) >= 8:
                        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
                        rows = table.query_selector_all('tr')
                        print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ: {len(rows)}")
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        for row in rows[1:]:
                            try:
                                cells = row.query_selector_all('td')
                                
                                if len(cells) >= 8:
                                    bin_text = cells[0].inner_text().strip()
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ë–ò–ù (—Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
                                    if bin_text.isdigit() and len(bin_text) >= 10:
                                        company_data = {
                                            'bin': bin_text,
                                            'name': cells[1].inner_text().strip(),
                                            'oked': cells[2].inner_text().strip(),
                                            'industry': cells[3].inner_text().strip(),
                                            'kato': cells[4].inner_text().strip(),
                                            'settlement': cells[5].inner_text().strip(),
                                            'krp': cells[6].inner_text().strip(),
                                            'company_size': cells[7].inner_text().strip().replace('\n', ' ')
                                        }
                                        
                                        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                                        for key, value in company_data.items():
                                            if isinstance(value, str):
                                                company_data[key] = ' '.join(value.split())
                                        
                                        companies.append(company_data)
                                        print(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–∞–Ω–∏—è: {company_data['name'][:50]}...")
                                    
                            except Exception as e:
                                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–æ–∫–∏: {e}")
                                continue
                        
                        if companies:
                            break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–∞–±–ª–∏—Ü—ã: {e}")
                    continue
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ç–∞–±–ª–∏—Ü: {e}")
        
        return companies

    def parse_region_page(self, url: str, region: str, page) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä–µ–≥–∏–æ–Ω–∞
        """
        companies = []
        
        try:
            print(f"\n–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}")
            page.goto(url, timeout=30000)
            time.sleep(2)  # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É
            
            companies = self.parse_company_table(page)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–≥–∏–æ–Ω–µ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–µ
            for company in companies:
                company['region'] = region
                company['source_url'] = url
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        
        return companies

    def parse_all_regions(self):
        """
        –ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
        """
        print("üá∞üáø –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞ KazDATA")
        print("=" * 60)
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            page = browser.new_page()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º User-Agent
            page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã
                print("–ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤...")
                page.goto(self.catalog_url, timeout=30000)
                time.sleep(2)
                
                region_links = self.extract_region_links(page)
                print(f"\n–ù–∞–π–¥–µ–Ω–æ —Ä–µ–≥–∏–æ–Ω–æ–≤: {len(region_links)}")
                
                # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π —Ä–µ–≥–∏–æ–Ω
                total_companies = 0
                
                for region, urls in region_links.items():
                    print(f"\nüìç –†–µ–≥–∏–æ–Ω: {region.upper()}")
                    region_companies = []
                    
                    for url in urls:
                        companies = self.parse_region_page(url, region, page)
                        region_companies.extend(companies)
                        time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                    if region_companies:
                        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ë–ò–ù
                        unique_companies = {}
                        for company in region_companies:
                            bin_number = company['bin']
                            if bin_number not in unique_companies:
                                unique_companies[bin_number] = company
                        
                        region_companies = list(unique_companies.values())
                        self.companies_by_region[region] = region_companies
                        
                        total_companies += len(region_companies)
                        print(f"‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –≤ {region}: {len(region_companies)}")
                    else:
                        print(f"‚ùå –ö–æ–º–ø–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {region}")
                
                print("\n" + "=" * 60)
                print(f"üéØ –ò–¢–û–ì–ò –ü–ê–†–°–ò–ù–ì–ê:")
                print(f"   –†–µ–≥–∏–æ–Ω–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(region_links)}")
                print(f"   –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ: {total_companies}")
                
            except Exception as e:
                print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            finally:
                browser.close()

    def save_data(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ CSV —Ñ–∞–π–ª—ã
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        data_dir = 'data/regions'
        os.makedirs(data_dir, exist_ok=True)
        
        fieldnames = [
            'bin', 'name', 'oked', 'industry', 'kato', 
            'settlement', 'krp', 'company_size', 'region', 'source_url'
        ]
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é /{data_dir}/")
        print("-" * 50)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        for region, companies in self.companies_by_region.items():
            if companies:
                filename = f"{data_dir}/{region}_companies.csv"
                
                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(companies)
                
                print(f"‚úÖ {region}_companies.csv - {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ –∫–æ–º–ø–∞–Ω–∏—è–º–∏
        all_companies = []
        for companies in self.companies_by_region.values():
            all_companies.extend(companies)
        
        if all_companies:
            filename = "data/all_kazakhstan_companies.csv"
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_companies)
            
            print(f"\n‚úÖ {filename} - {len(all_companies)} –∫–æ–º–ø–∞–Ω–∏–π")

def main():
    parser = KazDataCatalogParser()
    parser.parse_all_regions()
    parser.save_data()

if __name__ == "__main__":
    main() 